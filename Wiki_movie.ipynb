{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13e9853d-4bc4-4fa6-82ec-3bab35375963",
   "metadata": {},
   "source": [
    "# Step 1: Install and Load Libraries\n",
    "Ο στόχος του κώδικα είναι να διαβάσει τα δεδομένα από ένα dataset CSV που ονομάζεται wiki_movie_plots_deduped.csv και να τα προετοιμάσει για περαιτέρω επεξεργασία και ανάλυση.Αρχικά κάνουμε install στο environment μας τα πακέτα nltκ και pandas.Ο κώδικας επίσης κατεβάζει τα πακέτα stopwords και punkt του NLTK, που απαιτούνται για την επεξεργασία κειμένου, και χρησιμοποιεί έναν stemmer, συγκεκριμένα τον PorterStemmer, για να μετατρέπει λέξεις στις βασικές ρίζες τους. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2f43eb8-07d5-4174-ae9f-dc1ee714bf04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Release Year                             Title Origin/Ethnicity  \\\n",
       "0          1901            Kansas Saloon Smashers         American   \n",
       "1          1901     Love by the Light of the Moon         American   \n",
       "2          1901           The Martyred Presidents         American   \n",
       "3          1901  Terrible Teddy, the Grizzly King         American   \n",
       "4          1902            Jack and the Beanstalk         American   \n",
       "\n",
       "                             Director Cast    Genre  \\\n",
       "0                             Unknown  NaN  unknown   \n",
       "1                             Unknown  NaN  unknown   \n",
       "2                             Unknown  NaN  unknown   \n",
       "3                             Unknown  NaN  unknown   \n",
       "4  George S. Fleming, Edwin S. Porter  NaN  unknown   \n",
       "\n",
       "                                           Wiki Page  \\\n",
       "0  https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1  https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2  https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3  https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4  https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "\n",
       "                                                Plot  \n",
       "0  A bartender is working at a saloon, serving dr...  \n",
       "1  The moon, painted with a smiling face hangs ov...  \n",
       "2  The film, just over a minute long, is composed...  \n",
       "3  Lasting just 61 seconds and consisting of two ...  \n",
       "4  The earliest known adaptation of the classic f...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Install libraries if not already installed\n",
    "#!pip install pandas nltk\n",
    "#!pip install scikit-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Download NLTK stopwords and punkt tokenizer\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"punkt\")\n",
    "\n",
    "# Initialize stop words and stemmer\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# Load the CSV dataset\n",
    "dataset_path = \"wiki_movie_plots_deduped.csv\"\n",
    "movies_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows to verify the dataset\n",
    "movies_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dafc47-2c86-4257-a663-fc5887049acf",
   "metadata": {},
   "source": [
    "# Step 10.1 Load CISI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd831782-c70f-4fbe-b31c-2ade61baad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset:\n",
      "- Number of documents: 1460\n",
      "- Number of queries: 112\n",
      "- Number of queries with relevance data: 76\n"
     ]
    }
   ],
   "source": [
    "def load_cisi_dataset():\n",
    "    # File paths (modify based on file locations)\n",
    "    cisi_docs_path = \"CISI.ALL\"  # Contains documents\n",
    "    cisi_queries_path = \"CISI.QRY\"  # Contains queries\n",
    "    cisi_rels_path = \"CISI.REL\"  # Contains relevance judgments\n",
    "    \n",
    "    # Load documents\n",
    "    documents = {}\n",
    "    with open(cisi_docs_path, \"r\") as file:\n",
    "        raw_data = file.read().split(\".I\")  # Split by the document section header\n",
    "        for entry in raw_data[1:]:  # Skip the first entry, as it's empty or unnecessary\n",
    "            lines = entry.strip().split(\"\\n\")\n",
    "            # Ensure there's an ID to extract, skip over anything that doesn't look like an ID\n",
    "            try:\n",
    "                doc_id = int(lines[0].strip())  # Get the first line as the document ID\n",
    "            except ValueError:\n",
    "                continue  # Skip invalid lines\n",
    "            content = \"\\n\".join(lines[1:]).split(\".X\")[0].strip()  # Extract document content up to .X\n",
    "            documents[doc_id] = content\n",
    "\n",
    "    # Load queries\n",
    "    queries = {}\n",
    "    with open(cisi_queries_path, \"r\") as file:\n",
    "        raw_data = file.read().split(\".I\")\n",
    "        for entry in raw_data[1:]:\n",
    "            lines = entry.strip().split(\"\\n\")\n",
    "            # Handle cases where the first line might not be a valid integer ID\n",
    "            try:\n",
    "                query_id = int(lines[0].strip())\n",
    "            except ValueError:\n",
    "                continue  # Skip invalid lines\n",
    "            content = \"\\n\".join(lines[1:]).split(\".W\")[1].strip()  # Query content after \".W\"\n",
    "            queries[query_id] = content\n",
    "\n",
    "    # Load relevance judgments\n",
    "    relevances = {}\n",
    "    with open(cisi_rels_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # Handle lines with mixed data, non-integer values, or irrelevant rows\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 2:  # Skip lines with fewer than 2 entries (invalid entries)\n",
    "                continue\n",
    "            try:\n",
    "                query_id = int(parts[0])\n",
    "                doc_id = int(parts[1])  # Only convert the first two parts\n",
    "            except ValueError:\n",
    "                continue  # Skip any lines with invalid number conversions\n",
    "            relevances.setdefault(query_id, []).append(doc_id)\n",
    "\n",
    "    return documents, queries, relevances\n",
    "\n",
    "# Load the dataset\n",
    "documents, queries, relevances = load_cisi_dataset()\n",
    "\n",
    "# Check the loaded data\n",
    "print(\"Loaded dataset:\")\n",
    "print(f\"- Number of documents: {len(documents)}\")\n",
    "print(f\"- Number of queries: {len(queries)}\")\n",
    "print(f\"- Number of queries with relevance data: {len(relevances)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c96d6-4308-4e98-b45d-def996013ab3",
   "metadata": {},
   "source": [
    "# step 10.2 Preprocess CISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5ce7817f-3c3b-4cdf-a77f-b73b979ed071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'doc_id': 1, 'content': ['18', 'edit', 'dewey', 'decim', 'classif', 'comaromi', 'jp', 'w', 'present', 'studi', 'histori', 'dewey', 'decim', 'classif', 'first', 'edit', 'ddc', 'publish', '1876', 'eighteenth', 'edit', '1971', 'futur', 'edit', 'continu', 'appear', 'need', 'spite', 'ddc', 'long', 'healthi', 'life', 'howev', 'full', 'stori', 'never', 'told', 'biographi', 'dewey', 'briefli', 'describ', 'system', 'first', 'attempt', 'provid', 'detail', 'histori', 'work', 'spur', 'growth', 'librarianship', 'countri', 'abroad']}, {'doc_id': 2, 'content': ['use', 'made', 'technic', 'librari', 'slater', 'w', 'report', 'analysi', '6300', 'act', 'use', '104', 'technic', 'librari', 'unit', 'kingdom', 'librari', 'use', 'one', 'aspect', 'wider', 'pattern', 'inform', 'use', 'inform', 'transfer', 'librari', 'restrict', 'use', 'document', 'take', 'account', 'document', 'use', 'outsid', 'librari', 'still', 'less', 'inform', 'transfer', 'oral', 'person', 'person', 'librari', 'act', 'channel', 'proport', 'situat', 'inform', 'transfer', 'take', 'technic', 'inform', 'transfer', 'whole', 'doubt', 'proport', 'major', 'one', 'user', 'technic', 'inform', 'particularli', 'technolog', 'rather', 'scienc', 'visit', 'librari', 'rare', 'reli', 'desk', 'collect', 'handbook', 'current', 'period', 'person', 'contact', 'colleagu', 'peopl', 'organ', 'even', 'regular', 'librari', 'user', 'also', 'receiv', 'inform', 'way']}]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # Remove anything that's not a letter or a digit\n",
    "    \n",
    "    # Tokenize and remove stopwords, then apply stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Process documents from CISI dataset (make sure 'documents' is a dictionary)\n",
    "processed_documents = []\n",
    "\n",
    "# Processing each document in the CISI dataset\n",
    "for doc_id, doc_content in documents.items():\n",
    "    try:\n",
    "        # Preprocess document content\n",
    "        processed_doc = preprocess_text(doc_content)\n",
    "        processed_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"content\": processed_doc  # Store preprocessed content\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {doc_id}: {e}\")\n",
    "\n",
    "# Optionally limit to the first 100 documents\n",
    "processed_documents = processed_documents[:500]\n",
    "\n",
    "# Save processed data to JSON file\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Checking the saved output\n",
    "#print(f\"Processed data for {len(processed_documents)} documents.\")\n",
    "\n",
    "# Display first few processed documents\n",
    "print(processed_documents[:2])  # Display first processed document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0e9b7e-e352-4acb-8f50-40a6ac15195d",
   "metadata": {},
   "source": [
    "# Step 2: Save document on Json \n",
    "Ο κώδικας επεξεργάζεται το σύνολο δεδομένων με πλοκές ταινιών, εξάγοντας τίτλους και περιλήψεις σε μια λίστα από πεδία title και content. Στη συνέχεια, τα δεδομένα αυτά αποθηκεύονται σε ένα αρχείο JSON (all_movie_plots_data.json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "414b6f1b-50cc-4479-a05e-19183e478fb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Kansas Saloon Smashers',\n",
       "  'content': \"A bartender is working at a saloon, serving drinks to customers. After he fills a stereotypically Irish man's bucket with beer, Carrie Nation and her followers burst inside. They assault the Irish man, pulling his hat over his eyes and then dumping the beer over his head. The group then begin wrecking the bar, smashing the fixtures, mirrors, and breaking the cash register. The bartender then sprays seltzer water in Nation's face before a group of policemen appear and order everybody to leave.[1]\"}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all movie titles and plots from the dataset\n",
    "def get_all_movie_plots(df):\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        documents.append({\n",
    "            \"title\": row['Title'],\n",
    "            \"content\": row['Plot']\n",
    "        })\n",
    "    return documents\n",
    "\n",
    "# Fetch all movie plots from the dataset\n",
    "documents = get_all_movie_plots(movies_df)\n",
    "\n",
    "# Save the processed documents to a JSON file\n",
    "with open(\"all_movie_plots_data.json\", \"w\") as f:\n",
    "    json.dump(documents, f)\n",
    "\n",
    "# Display a sample of the documents to verify content\n",
    "documents[:1]  # Show the first document as an example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f63d47b-7818-469f-ae19-02ede8f0bc39",
   "metadata": {},
   "source": [
    "# Step 3: Processes the documents\n",
    "Ο κώδικας εφαρμόζει προεπεξεργασία στις πρώτες 200 εγγραφές του Dataset, αφαιρώντας ειδικούς χαρακτήρες, χαμηλώνοντας τους χαρακτήρες, αφαιρώντας stop words και εφαρμόζοντας stemming στις λέξεις. Τα επεξεργασμένα δεδομένα αποθηκεύονται σε αρχείο JSON (processed_data.json), ενώ γίνεται διαχείριση σφαλμάτων για τυχόν προβλήματα επεξεργασίας."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1c850bf-5ee1-4077-a136-1060f48e67b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Kansas Saloon Smashers',\n",
       "  'content': ['bartend',\n",
       "   'work',\n",
       "   'saloon',\n",
       "   'serv',\n",
       "   'drink',\n",
       "   'custom',\n",
       "   'fill',\n",
       "   'stereotyp',\n",
       "   'irish',\n",
       "   'man',\n",
       "   'bucket',\n",
       "   'beer',\n",
       "   'carri',\n",
       "   'nation',\n",
       "   'follow',\n",
       "   'burst',\n",
       "   'insid',\n",
       "   'assault',\n",
       "   'irish',\n",
       "   'man',\n",
       "   'pull',\n",
       "   'hat',\n",
       "   'eye',\n",
       "   'dump',\n",
       "   'beer',\n",
       "   'head',\n",
       "   'group',\n",
       "   'begin',\n",
       "   'wreck',\n",
       "   'bar',\n",
       "   'smash',\n",
       "   'fixtur',\n",
       "   'mirror',\n",
       "   'break',\n",
       "   'cash',\n",
       "   'regist',\n",
       "   'bartend',\n",
       "   'spray',\n",
       "   'seltzer',\n",
       "   'water',\n",
       "   'nation',\n",
       "   'face',\n",
       "   'group',\n",
       "   'policemen',\n",
       "   'appear',\n",
       "   'order',\n",
       "   'everybodi',\n",
       "   'leave1']}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize, remove stop words, and apply stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Limit processing to the first 200 documents\n",
    "processed_documents = []\n",
    "\n",
    "for i, doc in enumerate(documents[:200]):  # Only process the first 200 articles\n",
    "    try:\n",
    "        processed_documents.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": preprocess_text(doc[\"content\"])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i + 1}: {e}\")\n",
    "       \n",
    "# Save processed documents incrementally\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Display a sample to verify\n",
    "processed_documents[:1]  # Show the first processed document as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8117d3d1-ded6-4c55-82d9-17c727c41669",
   "metadata": {},
   "source": [
    "# Step 3.Α: Processes the documents (Includes stopwords)\n",
    "Ο κώδικας εφαρμόζει προεπεξεργασία στις πρώτες 200 εγγραφές του Dataset (δεν αφαιρώνται τα stop words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4af75b28-49b7-419d-80aa-17ed071c3be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Kansas Saloon Smashers',\n",
       "  'content': ['a',\n",
       "   'bartender',\n",
       "   'is',\n",
       "   'working',\n",
       "   'at',\n",
       "   'a',\n",
       "   'saloon',\n",
       "   'serving',\n",
       "   'drinks',\n",
       "   'to',\n",
       "   'customers',\n",
       "   'after',\n",
       "   'he',\n",
       "   'fills',\n",
       "   'a',\n",
       "   'stereotypically',\n",
       "   'irish',\n",
       "   'mans',\n",
       "   'bucket',\n",
       "   'with',\n",
       "   'beer',\n",
       "   'carrie',\n",
       "   'nation',\n",
       "   'and',\n",
       "   'her',\n",
       "   'followers',\n",
       "   'burst',\n",
       "   'inside',\n",
       "   'they',\n",
       "   'assault',\n",
       "   'the',\n",
       "   'irish',\n",
       "   'man',\n",
       "   'pulling',\n",
       "   'his',\n",
       "   'hat',\n",
       "   'over',\n",
       "   'his',\n",
       "   'eyes',\n",
       "   'and',\n",
       "   'then',\n",
       "   'dumping',\n",
       "   'the',\n",
       "   'beer',\n",
       "   'over',\n",
       "   'his',\n",
       "   'head',\n",
       "   'the',\n",
       "   'group',\n",
       "   'then',\n",
       "   'begin',\n",
       "   'wrecking',\n",
       "   'the',\n",
       "   'bar',\n",
       "   'smashing',\n",
       "   'the',\n",
       "   'fixtures',\n",
       "   'mirrors',\n",
       "   'and',\n",
       "   'breaking',\n",
       "   'the',\n",
       "   'cash',\n",
       "   'register',\n",
       "   'the',\n",
       "   'bartender',\n",
       "   'then',\n",
       "   'sprays',\n",
       "   'seltzer',\n",
       "   'water',\n",
       "   'in',\n",
       "   'nations',\n",
       "   'face',\n",
       "   'before',\n",
       "   'a',\n",
       "   'group',\n",
       "   'of',\n",
       "   'policemen',\n",
       "   'appear',\n",
       "   'and',\n",
       "   'order',\n",
       "   'everybody',\n",
       "   'to',\n",
       "   'leave1']}]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing function: Includes stopwords\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize and retain stopwords\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Limit processing to the first 200 documents\n",
    "processed_documents = []\n",
    "\n",
    "for i, doc in enumerate(documents[:200]):  # Only process the first 200 articles\n",
    "    try:\n",
    "        processed_documents.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": preprocess_text(doc[\"content\"])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i + 1}: {e}\")\n",
    "       \n",
    "# Save processed documents incrementally\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Display a sample to verify\n",
    "processed_documents[:1]  # Show the first processed document as an example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bcf47-ee1f-4e65-bf36-e6cac5e9dfdf",
   "metadata": {},
   "source": [
    "# Step 3.Β: Processes the documents(Skips stemming)\n",
    "Ο κώδικας εφαρμόζει προεπεξεργασία στις πρώτες 200 εγγραφές του Dataset, δεν εφαρμόζονται stemming στις λέξεις."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9a1ad3a9-1983-4b66-adf2-c30c7ed7f89b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Kansas Saloon Smashers',\n",
       "  'content': ['bartender',\n",
       "   'working',\n",
       "   'saloon',\n",
       "   'serving',\n",
       "   'drinks',\n",
       "   'customers',\n",
       "   'fills',\n",
       "   'stereotypically',\n",
       "   'irish',\n",
       "   'mans',\n",
       "   'bucket',\n",
       "   'beer',\n",
       "   'carrie',\n",
       "   'nation',\n",
       "   'followers',\n",
       "   'burst',\n",
       "   'inside',\n",
       "   'assault',\n",
       "   'irish',\n",
       "   'man',\n",
       "   'pulling',\n",
       "   'hat',\n",
       "   'eyes',\n",
       "   'dumping',\n",
       "   'beer',\n",
       "   'head',\n",
       "   'group',\n",
       "   'begin',\n",
       "   'wrecking',\n",
       "   'bar',\n",
       "   'smashing',\n",
       "   'fixtures',\n",
       "   'mirrors',\n",
       "   'breaking',\n",
       "   'cash',\n",
       "   'register',\n",
       "   'bartender',\n",
       "   'sprays',\n",
       "   'seltzer',\n",
       "   'water',\n",
       "   'nations',\n",
       "   'face',\n",
       "   'group',\n",
       "   'policemen',\n",
       "   'appear',\n",
       "   'order',\n",
       "   'everybody',\n",
       "   'leave1']}]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preprocessing function: Skips stemming\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize and remove stopwords, without stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Limit processing to the first 200 documents\n",
    "processed_documents = []\n",
    "\n",
    "for i, doc in enumerate(documents[:200]):  # Only process the first 200 articles\n",
    "    try:\n",
    "        processed_documents.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": preprocess_text(doc[\"content\"])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i + 1}: {e}\")\n",
    "       \n",
    "# Save processed documents incrementally\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Display a sample to verify\n",
    "processed_documents[:1]  # Show the first processed document as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b91220-4599-4bc5-9f29-fa45f3b2df9c",
   "metadata": {},
   "source": [
    "# Step 4: Create inverted index\n",
    "Ο κώδικας δημιουργεί έναν αντίστροφο δείκτη , καταγράφοντας τη συχνότητα εμφάνισης κάθε λέξης σε κάθε έγγραφο. Αποθηκεύει τον δείκτη σε αρχείο JSON (inverted_index.json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc77d2e2-fe70-4011-8df6-f162885e753e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bartend': {0: 2, 192: 1}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty inverted index\n",
    "inverted_index = defaultdict(dict)\n",
    "\n",
    "# Populate the inverted index with term frequencies\n",
    "for doc_id, doc in enumerate(processed_documents):\n",
    "    for term in doc[\"content\"]:\n",
    "        if doc_id in inverted_index[term]:\n",
    "            inverted_index[term][doc_id] += 1\n",
    "        else:\n",
    "            inverted_index[term][doc_id] = 1\n",
    "\n",
    "# Save the inverted index to a JSON file\n",
    "with open(\"inverted_index.json\", \"w\") as f:\n",
    "    json.dump(inverted_index, f)\n",
    "\n",
    "# Display a small part of the inverted index to verify content\n",
    "dict(list(inverted_index.items())[:1])  # Show the first term in the index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "face4414-92bd-42d9-8546-64cb477150c3",
   "metadata": {},
   "source": [
    "# Step 5:Boolean Retrieval\n",
    "Ο κώδικας επιτρέπει αναζητήσεις Boolean πάνω στις πλοκές ταινιών. Ανάλυσε το ερώτημα σε μικρές λέξεις (terms) και εφαρμόζει τους λογικούς τελεστές \"and\", \"or\" και \"not\" για να ανακτήσει έγγραφα που πληρούν τα κριτήρια"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f5ba9e5b-5696-4c06-a095-1aca9447a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the query with Boolean operators\n",
    "def parse_query(query):\n",
    "    terms = query.lower().split()\n",
    "    tokens = []\n",
    "    operators = {\"and\", \"or\", \"not\"}\n",
    "    \n",
    "   \n",
    "\n",
    "# Boolean retrieval function\n",
    "def boolean_retrieval(query_tokens):\n",
    "    result_set = set(range(len(processed_documents)))  # Start with all documents\n",
    "    current_operation = \"and\"\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in {\"and\", \"or\", \"not\"}:\n",
    "            current_operation = token\n",
    "        else:\n",
    "            matching_docs = set(inverted_index.get(token, {}).keys())\n",
    "            if current_operation == \"and\":\n",
    "                result_set &= matching_docs\n",
    "            elif current_operation == \"or\":\n",
    "                result_set |= matching_docs\n",
    "            elif current_operation == \"not\":\n",
    "                result_set -= matching_docs\n",
    "    \n",
    "    return list(result_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e6fad6-85b0-4625-bba8-80727cdbe617",
   "metadata": {},
   "source": [
    "# Step 6:TF-IDF Ranking\n",
    "Ο κώδικας υπολογίζει το TF-IDF για κάθε όρο ενός ερωτήματος σε σχετικά έγγραφα, συνδυάζοντας τη συχνότητα του όρου σε ένα έγγραφο (TF) και τη σπανιότητά του σε όλα τα έγγραφα (IDF). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76e60751-fc45-40c6-804a-8bcead8778c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF weight for a term in a document\n",
    "def compute_tf_idf(term, doc_id):\n",
    "    term_frequency = inverted_index[term].get(doc_id, 0)\n",
    "    if term_frequency == 0:\n",
    "        return 0\n",
    "    document_frequency = len(inverted_index[term])\n",
    "    inverse_document_frequency = math.log(len(processed_documents) / (1 + document_frequency))\n",
    "    return term_frequency * inverse_document_frequency\n",
    "\n",
    "# Rank results by TF-IDF scores\n",
    "def rank_results_tf_idf(query_tokens, result_docs):\n",
    "    doc_scores = {}\n",
    "    for doc_id in result_docs:\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            if term not in {\"and\", \"or\", \"not\"}:\n",
    "                score += compute_tf_idf(term, doc_id)\n",
    "        doc_scores[doc_id] = score\n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabf99a6-611b-4fe4-9ee2-7b32f31e9514",
   "metadata": {},
   "source": [
    "# Step 7:BM25 Ranking\n",
    "Ο κώδικας υπολογίζει τη βαθμολογία BM25 για έγγραφα σε σχέση με ένα ερώτημα, λαμβάνοντας υπόψη τη συχνότητα των όρων (TF), τη σπανιότητά τους (IDF) και το μήκος των εγγράφων με κανονικοποίηση. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e66731ad-7495-4f19-918e-a90d656e36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 parameters\n",
    "k1 = 1.5  # Term frequency saturation parameter\n",
    "b = 0.75  # Length normalization parameter\n",
    "\n",
    "# Precompute document lengths and average length\n",
    "document_lengths = [len(doc[\"content\"]) for doc in processed_documents]\n",
    "avg_doc_length = sum(document_lengths) / len(document_lengths)\n",
    "\n",
    "# Function to compute BM25 score for a term in a document\n",
    "def compute_bm25(term, doc_id):\n",
    "    term_frequency = inverted_index[term].get(doc_id, 0)\n",
    "    document_frequency = len(inverted_index[term])\n",
    "    N = len(processed_documents)\n",
    "    \n",
    "    # Calculate IDF component with BM25 modification\n",
    "    idf = math.log((N - document_frequency + 0.5) / (document_frequency + 0.5) + 1)\n",
    "    \n",
    "    # Calculate the BM25 term score\n",
    "    doc_length = document_lengths[doc_id]\n",
    "    tf_component = (term_frequency * (k1 + 1)) / (term_frequency + k1 * (1 - b + b * (doc_length / avg_doc_length)))\n",
    "    \n",
    "    return idf * tf_component\n",
    "\n",
    "# Rank results by BM25 scores\n",
    "def rank_results_bm25(query_tokens, result_docs):\n",
    "    doc_scores = {}\n",
    "    for doc_id in result_docs:\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            if term not in {\"and\", \"or\", \"not\"}:\n",
    "                score += compute_bm25(term, doc_id)\n",
    "        doc_scores[doc_id] = score\n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efb403d-9e36-44fe-800c-2481fe3e3866",
   "metadata": {},
   "source": [
    "# Step 8: VSM Ranking\n",
    "Ο κώδικας υλοποιεί μοντέλο Vector Space Model για την κατάταξη εγγράφων βάσει της συσχέτισής τους με το ερώτημα. Χρησιμοποιεί τα αποτελέσματα της Boolean αναζήτησης για να φιλτράρει σχετικά έγγραφα, μετατρέπει τα έγγραφα και το ερώτημα σε διανύσματα TF-IDF με το TfidfVectorizer, και υπολογίζει την ομοιότητα συνημιτόνου (cosine similarity) μεταξύ του διανύσματος του ερωτήματος και των διανυσμάτων των εγγράφων"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2ec151db-2c52-43d9-89d5-e4f87b4db177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def vector_space_model(query_tokens):\n",
    "   \n",
    "    # Filter documents using Boolean Retrieval results\n",
    "    result_docs = boolean_retrieval(query_tokens)\n",
    "    if not result_docs:  # No documents matched\n",
    "        print(\"No documents match the query.\")\n",
    "        return []\n",
    "\n",
    "    # Extract text for the filtered documents\n",
    "    corpus = [\" \".join(processed_documents[doc_id][\"content\"]) for doc_id in result_docs]\n",
    "    query = \" \".join(query_tokens)  # Represent query as a single string\n",
    "\n",
    "    # Vectorize using TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_vectors = vectorizer.fit_transform(corpus + [query])  # Last row is the query vector\n",
    "\n",
    "    document_vectors = all_vectors[:-1]  # Exclude query vector (last)\n",
    "    query_vector = all_vectors[-1]  # Query vector\n",
    "\n",
    "    # Compute cosine similarity using sklearn's cosine_similarity\n",
    "    similarity_scores = cosine_similarity(query_vector, document_vectors).flatten()\n",
    "\n",
    "    # Map scores to document IDs\n",
    "    scored_results = [(result_docs[i], score) for i, score in enumerate(similarity_scores)]\n",
    "\n",
    "    # Sort documents by similarity score in descending order\n",
    "    sorted_results = sorted(scored_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98337430-2d1e-4cbd-a8b0-194a743edaed",
   "metadata": {},
   "source": [
    "# Step 9: Search Function\n",
    "Η συνάρτηση search_documents εκτελεί αναζήτηση σε έγγραφα με βάση τέσσερις αλγορίθμους: Boolean Retrieval, TF-IDF Ranking, BM25 Ranking και Vector Space Model (VSM). Για το κάθε ερώτημα, τα tokens προεπεξεργάζονται και η μέθοδος που επιλέγεται επιστρέφει τα αποτελέσματα ταξινομημένα. Ανάλογα με τον αλγόριθμο, εμφανίζονται οι τίτλοι εγγράφων και (όπου εφαρμόζεται) οι βαθμολογίες, μέσω όμορφης παρουσίασης με χρήση της βιβλιοθήκης tabulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c88fa751-39ed-48be-a2e9-6d8ccec2faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, algorithm=\"boolean\"):\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    if algorithm == \"boolean\":\n",
    "        # Boolean Retrieval: Display document titles\n",
    "        results = boolean_retrieval(query_tokens)\n",
    "        sorted_results = sorted(results)  # Sort by document ID (ascending)\n",
    "        data = [[doc_id, documents[doc_id]['title']] for doc_id in sorted_results]\n",
    "        print(\"\\nBoolean Retrieval Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\"], tablefmt=\"grid\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"tf-idf\":\n",
    "        # TF-IDF Ranking: Display document titles and scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_tf_idf(query_tokens, result_docs)\n",
    "        # Sort by TF-IDF score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "        data = [[doc_id, documents[doc_id]['title'], score] for doc_id, score in sorted_results]\n",
    "        print(\"\\nTF-IDF Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"bm25\":\n",
    "        # BM25 Ranking: Display document titles and BM25 scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_bm25(query_tokens, result_docs)\n",
    "        # Sort by BM25 score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "        data = [[doc_id, documents[doc_id]['title'], score] for doc_id, score in sorted_results]\n",
    "        print(\"\\nOkapi BM25 Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"BM25 Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"vsm\":\n",
    "        # VSM Ranking: Display document titles and similarity scores\n",
    "        ranked_results = vector_space_model(query_tokens)\n",
    "        # Already sorted by VSM score (descending) within `vector_space_model`\n",
    "        data = [[doc_id, documents[doc_id]['title'], score] for doc_id, score in ranked_results[:10]]  # Top 10 results\n",
    "        print(\"\\nVSM Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"VSM Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return ranked_results\n",
    "\n",
    "    else:\n",
    "        print(\"Unsupported algorithm selected.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e7f38b-7d82-488d-bf59-909486b90f67",
   "metadata": {},
   "source": [
    "# Step 10.3: Search Fuction for CISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7dd5e802-b9a6-437b-b9df-e294c788eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, algorithm=\"boolean\"):\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    if algorithm == \"boolean\":\n",
    "        # Boolean Retrieval: Display document titles (or content in case of CISI)\n",
    "        results = boolean_retrieval(query_tokens)\n",
    "        sorted_results = sorted(results)  # Sort by document ID (ascending)\n",
    "\n",
    "        data = []\n",
    "        for doc_id in sorted_results:\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:100]  # First 100 characters or a preview\n",
    "                data.append([doc_id, title])\n",
    "\n",
    "        print(\"\\nBoolean Retrieval Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\"], tablefmt=\"grid\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"tf-idf\":\n",
    "        # TF-IDF Ranking: Display document titles and scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_tf_idf(query_tokens, result_docs)\n",
    "        # Sort by TF-IDF score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        data = []\n",
    "        for doc_id, score in sorted_results:\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:500]  # First 500 characters or a preview\n",
    "                data.append([doc_id, title, score])\n",
    "\n",
    "        print(\"\\nTF-IDF Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"bm25\":\n",
    "        # BM25 Ranking: Display document titles and BM25 scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_bm25(query_tokens, result_docs)\n",
    "        # Sort by BM25 score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        data = []\n",
    "        for doc_id, score in sorted_results:\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:500]  # First 500 characters or a preview\n",
    "                data.append([doc_id, title, score])\n",
    "\n",
    "        print(\"\\nOkapi BM25 Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"BM25 Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"vsm\":\n",
    "        # VSM Ranking: Display document titles and similarity scores\n",
    "        ranked_results = vector_space_model(query_tokens)\n",
    "        # Already sorted by VSM score (descending) within `vector_space_model`\n",
    "        \n",
    "        data = []\n",
    "        for doc_id, score in ranked_results[:10]:  # Top 10 results\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:500]  # First 500 characters or a preview\n",
    "                data.append([doc_id, title, score])\n",
    "\n",
    "        print(\"\\nVSM Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"VSM Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return ranked_results\n",
    "\n",
    "    else:\n",
    "        print(\"Unsupported algorithm selected.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b1fa1-2b04-4c8a-87e3-2072571ab872",
   "metadata": {},
   "source": [
    "# Step 11: Interactive Query Loop\n",
    "Ο χρήστης επιλέγει αλγόριθμο (Boolean Retrieval, TF-IDF, BM25, ή VSM) και εισάγει ένα ερώτημα. Η επιλογή αλγόριθμου κατευθύνει στη συνάρτηση search_documents, η οποία επιστρέφει αποτελέσματα βασισμένα στην αντίστοιχη μέθοδο. Το πρόγραμμα συνεχίζεται έως ότου ο χρήστης επιλέξει την έξοδο, με έλεγχο εγκυρότητας για τις επιλογές και τα ερωτήματα."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11ebfb28-b203-47e8-a35b-e7a5f136ffdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Retrieval System ---\n",
      "Choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF\n",
      "3. Okapi BM25\n",
      "4. vsm\n",
      "0. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (0 to exit):  1\n",
      "Enter your search query:  Mother and Father\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You selected: Boolean Retrieval\n",
      "\n",
      "Boolean Retrieval Results:\n",
      "+---------------+---------------------------+\n",
      "|   Document ID | Title                     |\n",
      "+===============+===========================+\n",
      "|            16 | The Adventures of Dollie  |\n",
      "+---------------+---------------------------+\n",
      "|            47 | The New York Hat          |\n",
      "+---------------+---------------------------+\n",
      "|            51 | Atlantis                  |\n",
      "+---------------+---------------------------+\n",
      "|           108 | Birth of a Nation         |\n",
      "+---------------+---------------------------+\n",
      "|           133 | The Bondman               |\n",
      "+---------------+---------------------------+\n",
      "|           174 | The Little American       |\n",
      "+---------------+---------------------------+\n",
      "|           177 | The Mate of the Sally Ann |\n",
      "+---------------+---------------------------+\n",
      "|           178 | A Modern Musketeer        |\n",
      "+---------------+---------------------------+\n",
      "|           181 | The Poor Little Rich Girl |\n",
      "+---------------+---------------------------+\n",
      "\n",
      "--- Document Retrieval System ---\n",
      "Choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF\n",
      "3. Okapi BM25\n",
      "4. vsm\n",
      "0. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (0 to exit):  2\n",
      "Enter your search query:  Mother and Father\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You selected: TF-IDF Ranking\n",
      "\n",
      "TF-IDF Ranked Results:\n",
      "+---------------+---------------------------+---------+\n",
      "|   Document ID | Title                     |   Score |\n",
      "+===============+===========================+=========+\n",
      "|            16 | The Adventures of Dollie  |  9.3126 |\n",
      "+---------------+---------------------------+---------+\n",
      "|           177 | The Mate of the Sally Ann |  8.7702 |\n",
      "+---------------+---------------------------+---------+\n",
      "|           178 | A Modern Musketeer        |  8.7702 |\n",
      "+---------------+---------------------------+---------+\n",
      "|           133 | The Bondman               |  6.6908 |\n",
      "+---------------+---------------------------+---------+\n",
      "|            47 | The New York Hat          |  6.6908 |\n",
      "+---------------+---------------------------+---------+\n",
      "|            51 | Atlantis                  |  6.6908 |\n",
      "+---------------+---------------------------+---------+\n",
      "|           108 | Birth of a Nation         |  3.6166 |\n",
      "+---------------+---------------------------+---------+\n",
      "|           174 | The Little American       |  3.6166 |\n",
      "+---------------+---------------------------+---------+\n",
      "|           181 | The Poor Little Rich Girl |  3.6166 |\n",
      "+---------------+---------------------------+---------+\n",
      "\n",
      "--- Document Retrieval System ---\n",
      "Choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF\n",
      "3. Okapi BM25\n",
      "4. vsm\n",
      "0. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (0 to exit):  3\n",
      "Enter your search query:  Mother and Father\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You selected: Okapi BM25\n",
      "\n",
      "Okapi BM25 Ranked Results:\n",
      "+---------------+---------------------------+--------------+\n",
      "|   Document ID | Title                     |   BM25 Score |\n",
      "+===============+===========================+==============+\n",
      "|            16 | The Adventures of Dollie  |       6.1143 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|           177 | The Mate of the Sally Ann |       5.4068 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|           133 | The Bondman               |       5.2794 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|            47 | The New York Hat          |       5.1109 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|           181 | The Poor Little Rich Girl |       4.0120 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|           178 | A Modern Musketeer        |       3.4283 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|            51 | Atlantis                  |       2.8490 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|           174 | The Little American       |       1.8995 |\n",
      "+---------------+---------------------------+--------------+\n",
      "|           108 | Birth of a Nation         |       1.1940 |\n",
      "+---------------+---------------------------+--------------+\n",
      "\n",
      "--- Document Retrieval System ---\n",
      "Choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF\n",
      "3. Okapi BM25\n",
      "4. vsm\n",
      "0. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (0 to exit):  4\n",
      "Enter your search query:  Mother and Father\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You selected: Vector Space Model (VSM)\n",
      "\n",
      "VSM Ranked Results:\n",
      "+---------------+---------------------------+-------------+\n",
      "|   Document ID | Title                     |   VSM Score |\n",
      "+===============+===========================+=============+\n",
      "|            16 | The Adventures of Dollie  |      0.1134 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|            47 | The New York Hat          |      0.0976 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|           177 | The Mate of the Sally Ann |      0.0885 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|           133 | The Bondman               |      0.0798 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|           181 | The Poor Little Rich Girl |      0.0666 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|           178 | A Modern Musketeer        |      0.0439 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|            51 | Atlantis                  |      0.0389 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|           174 | The Little American       |      0.0159 |\n",
      "+---------------+---------------------------+-------------+\n",
      "|           108 | Birth of a Nation         |      0.0128 |\n",
      "+---------------+---------------------------+-------------+\n",
      "\n",
      "--- Document Retrieval System ---\n",
      "Choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF\n",
      "3. Okapi BM25\n",
      "4. vsm\n",
      "0. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (0 to exit):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    while True:\n",
    "        print(\"\\n--- Document Retrieval System ---\")\n",
    "        print(\"Choose an algorithm:\")\n",
    "        print(\"1. Boolean Retrieval\")\n",
    "        print(\"2. TF-IDF\")\n",
    "        print(\"3. Okapi BM25\")\n",
    "        print(\"4. vsm\")\n",
    "        print(\"0. Exit\")\n",
    "\n",
    "        choice = input(\"Enter your choice (0 to exit): \").strip()\n",
    "\n",
    "        if choice == \"0\":\n",
    "            print(\"Exiting\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Enter your search query: \").strip()\n",
    "\n",
    "        if not query:\n",
    "            print(\"Query cannot be empty. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        if choice == \"1\":\n",
    "            print(\"\\nYou selected: Boolean Retrieval\")\n",
    "            search_documents(query, \"boolean\")\n",
    "        elif choice == \"2\":\n",
    "            print(\"\\nYou selected: TF-IDF Ranking\")\n",
    "            search_documents(query, \"tf-idf\")\n",
    "        elif choice == \"3\":\n",
    "            print(\"\\nYou selected: Okapi BM25\")\n",
    "            search_documents(query, \"bm25\")\n",
    "        elif choice == \"4\":\n",
    "            print(\"\\nYou selected: Vector Space Model (VSM)\")\n",
    "            search_documents(query, \"vsm\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 0 and 4.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bf5f82-2364-403d-996c-aaf6a0c7043d",
   "metadata": {},
   "source": [
    "# Step 10: Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "440c915f-98f7-452c-9285-234a5cffdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating with Boolean Retrieval:\n",
      "\n",
      "Evaluating query 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "\n",
      "Boolean Retrieval Results:\n",
      "+---------------+---------+\n",
      "| Document ID   | Title   |\n",
      "+===============+=========+\n",
      "+---------------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 2: How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n",
      "\n",
      "Boolean Retrieval Results:\n",
      "+---------------+---------+\n",
      "| Document ID   | Title   |\n",
      "+===============+=========+\n",
      "+---------------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 3: What is information science?  Give definitions where possible.\n",
      "\n",
      "Boolean Retrieval Results:\n",
      "+---------------+---------+\n",
      "| Document ID   | Title   |\n",
      "+===============+=========+\n",
      "+---------------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Mean Average Precision (MAP): 0.0000\n",
      "\n",
      "Evaluating with TF-IDF Retrieval:\n",
      "\n",
      "Evaluating query 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "\n",
      "TF-IDF Ranked Results:\n",
      "+---------------+---------+---------+\n",
      "| Document ID   | Title   | Score   |\n",
      "+===============+=========+=========+\n",
      "+---------------+---------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 2: How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n",
      "\n",
      "TF-IDF Ranked Results:\n",
      "+---------------+---------+---------+\n",
      "| Document ID   | Title   | Score   |\n",
      "+===============+=========+=========+\n",
      "+---------------+---------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 3: What is information science?  Give definitions where possible.\n",
      "\n",
      "TF-IDF Ranked Results:\n",
      "+---------------+---------+---------+\n",
      "| Document ID   | Title   | Score   |\n",
      "+===============+=========+=========+\n",
      "+---------------+---------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Mean Average Precision (MAP): 0.0000\n",
      "\n",
      "Evaluating with BM25 Retrieval:\n",
      "\n",
      "Evaluating query 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "\n",
      "Okapi BM25 Ranked Results:\n",
      "+---------------+---------+--------------+\n",
      "| Document ID   | Title   | BM25 Score   |\n",
      "+===============+=========+==============+\n",
      "+---------------+---------+--------------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 2: How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n",
      "\n",
      "Okapi BM25 Ranked Results:\n",
      "+---------------+---------+--------------+\n",
      "| Document ID   | Title   | BM25 Score   |\n",
      "+===============+=========+==============+\n",
      "+---------------+---------+--------------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 3: What is information science?  Give definitions where possible.\n",
      "\n",
      "Okapi BM25 Ranked Results:\n",
      "+---------------+---------+--------------+\n",
      "| Document ID   | Title   | BM25 Score   |\n",
      "+===============+=========+==============+\n",
      "+---------------+---------+--------------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Mean Average Precision (MAP): 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Sample queries (CISI dataset)\n",
    "queries = {\n",
    "    \"1\": \"What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\",\n",
    "    \"2\":\"How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\",\n",
    "    \"3\":\"What is information science?  Give definitions where possible.\"\n",
    "}\n",
    "\n",
    "# Sample relevance data \n",
    "# Format: query_id -> list of relevant document IDs (str)\n",
    "relevances = {\n",
    "    \"1\": [\"28\", \"35\", \"38\", \"42\"],  # Relevant document IDs for query 1\n",
    "    \"2\": [\"29\",\"68\",\"197\"],\n",
    "    \"3\": [\"60\", \"85\"]\n",
    "}\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_average_precision(retrieved_docs, relevant_docs):\n",
    "    num_relevant_retrieved = 0\n",
    "    ap = 0.0\n",
    "\n",
    "    for rank, doc_id in enumerate(retrieved_docs, start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            num_relevant_retrieved += 1\n",
    "            precision_at_rank = num_relevant_retrieved / rank\n",
    "            ap += precision_at_rank\n",
    "\n",
    "    if len(relevant_docs) > 0:\n",
    "        ap /= len(relevant_docs)\n",
    "    \n",
    "    return ap\n",
    "\n",
    "def evaluate_search_results(retrieved_docs, query_id, relevances):\n",
    "    relevant_docs = relevances[query_id]\n",
    "    tp = len(set(retrieved_docs) & set(relevant_docs))  # True Positives\n",
    "    fp = len(set(retrieved_docs) - set(relevant_docs))  # False Positives\n",
    "    fn = len(set(relevant_docs) - set(retrieved_docs))  # False Negatives\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_all_queries(queries, relevances, search_documents, algorithm=\"boolean\"):\n",
    "    map_score = 0\n",
    "    for query_id, query_content in queries.items():\n",
    "        print(f\"\\nEvaluating query {query_id}: {query_content}\")\n",
    "        \n",
    "        retrieved_docs = search_documents(query_content, algorithm=algorithm)\n",
    "\n",
    "        precision, recall, f1 = evaluate_search_results(retrieved_docs, query_id, relevances)\n",
    "        ap = compute_average_precision(retrieved_docs, relevances[query_id])\n",
    "        map_score += ap\n",
    "\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, Average Precision: {ap:.4f}\")\n",
    "\n",
    "    map_score /= len(queries)\n",
    "    print(f\"\\nMean Average Precision (MAP): {map_score:.4f}\")\n",
    "\n",
    "# Evaluation for each algorithm\n",
    "print(\"\\nEvaluating with Boolean Retrieval:\")\n",
    "evaluate_all_queries(queries, relevances, search_documents, algorithm=\"boolean\")\n",
    "print(\"\\nEvaluating with TF-IDF Retrieval:\")\n",
    "evaluate_all_queries(queries, relevances, search_documents, algorithm=\"tf-idf\")\n",
    "print(\"\\nEvaluating with BM25 Retrieval:\")\n",
    "evaluate_all_queries(queries, relevances, search_documents, algorithm=\"bm25\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
