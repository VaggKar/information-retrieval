{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d715651-b298-4989-825d-37f43fccdb28",
   "metadata": {},
   "source": [
    "# Step 1: Install and Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f43eb8-07d5-4174-ae9f-dc1ee714bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install libraries if not already installed\n",
    "#!pip install pandas nltk\n",
    "#!pip install scikit-learn\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# Download NLTK stopwords and punkt tokenizer\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"punkt\")\n",
    "\n",
    "# Initialize stop words and stemmer\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "# Load the CSV dataset\n",
    "dataset_path = \"wiki_movie_plots_deduped.csv\"\n",
    "movies_df = pd.read_csv(dataset_path)\n",
    "\n",
    "# Display the first few rows to verify the dataset\n",
    "#movies_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43dafc47-2c86-4257-a663-fc5887049acf",
   "metadata": {},
   "source": [
    "# Step 10.1 Load CISI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd831782-c70f-4fbe-b31c-2ade61baad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset:\n",
      "- Number of documents: 1460\n",
      "- Number of queries: 112\n",
      "- Number of queries with relevance data: 76\n"
     ]
    }
   ],
   "source": [
    "def load_cisi_dataset():\n",
    "    # File paths (modify based on file locations)\n",
    "    cisi_docs_path = \"CISI.ALL\"  # Contains documents\n",
    "    cisi_queries_path = \"CISI.QRY\"  # Contains queries\n",
    "    cisi_rels_path = \"CISI.REL\"  # Contains relevance judgments\n",
    "    \n",
    "    # Load documents\n",
    "    documents = {}\n",
    "    with open(cisi_docs_path, \"r\") as file:\n",
    "        raw_data = file.read().split(\".I\")  # Split by the document section header\n",
    "        for entry in raw_data[1:]:  # Skip the first entry, as it's empty or unnecessary\n",
    "            lines = entry.strip().split(\"\\n\")\n",
    "            # Ensure there's an ID to extract, skip over anything that doesn't look like an ID\n",
    "            try:\n",
    "                doc_id = int(lines[0].strip())  # Get the first line as the document ID\n",
    "            except ValueError:\n",
    "                continue  # Skip invalid lines\n",
    "            content = \"\\n\".join(lines[1:]).split(\".X\")[0].strip()  # Extract document content up to .X\n",
    "            documents[doc_id] = content\n",
    "\n",
    "    # Load queries\n",
    "    queries = {}\n",
    "    with open(cisi_queries_path, \"r\") as file:\n",
    "        raw_data = file.read().split(\".I\")\n",
    "        for entry in raw_data[1:]:\n",
    "            lines = entry.strip().split(\"\\n\")\n",
    "            # Handle cases where the first line might not be a valid integer ID\n",
    "            try:\n",
    "                query_id = int(lines[0].strip())\n",
    "            except ValueError:\n",
    "                continue  # Skip invalid lines\n",
    "            content = \"\\n\".join(lines[1:]).split(\".W\")[1].strip()  # Query content after \".W\"\n",
    "            queries[query_id] = content\n",
    "\n",
    "    # Load relevance judgments\n",
    "    relevances = {}\n",
    "    with open(cisi_rels_path, \"r\") as file:\n",
    "        for line in file:\n",
    "            # Handle lines with mixed data, non-integer values, or irrelevant rows\n",
    "            parts = line.strip().split()\n",
    "            if len(parts) < 2:  # Skip lines with fewer than 2 entries (invalid entries)\n",
    "                continue\n",
    "            try:\n",
    "                query_id = int(parts[0])\n",
    "                doc_id = int(parts[1])  # Only convert the first two parts\n",
    "            except ValueError:\n",
    "                continue  # Skip any lines with invalid number conversions\n",
    "            relevances.setdefault(query_id, []).append(doc_id)\n",
    "\n",
    "    return documents, queries, relevances\n",
    "\n",
    "# Load the dataset\n",
    "documents, queries, relevances = load_cisi_dataset()\n",
    "\n",
    "# Check the loaded data\n",
    "print(\"Loaded dataset:\")\n",
    "print(f\"- Number of documents: {len(documents)}\")\n",
    "print(f\"- Number of queries: {len(queries)}\")\n",
    "print(f\"- Number of queries with relevance data: {len(relevances)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c96d6-4308-4e98-b45d-def996013ab3",
   "metadata": {},
   "source": [
    "# step 10.2 Preprocess CISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ce7817f-3c3b-4cdf-a77f-b73b979ed071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)  # Remove anything that's not a letter or a digit\n",
    "    \n",
    "    # Tokenize and remove stopwords, then apply stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Process documents from CISI dataset (make sure 'documents' is a dictionary)\n",
    "processed_documents = []\n",
    "\n",
    "# Processing each document in the CISI dataset\n",
    "for doc_id, doc_content in documents.items():\n",
    "    try:\n",
    "        # Preprocess document content\n",
    "        processed_doc = preprocess_text(doc_content)\n",
    "        processed_documents.append({\n",
    "            \"doc_id\": doc_id,\n",
    "            \"content\": processed_doc  # Store preprocessed content\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {doc_id}: {e}\")\n",
    "\n",
    "# Optionally limit to the first 100 documents\n",
    "processed_documents = processed_documents[:500]\n",
    "\n",
    "# Save processed data to JSON file\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Checking the saved output\n",
    "#print(f\"Processed data for {len(processed_documents)} documents.\")\n",
    "\n",
    "# Display first few processed documents\n",
    "#print(processed_documents[:2])  # Display first processed document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762dc634-8fb6-4827-a59f-e1f0d2b429e5",
   "metadata": {},
   "source": [
    "# Step 2: Save document on Json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "414b6f1b-50cc-4479-a05e-19183e478fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all movie titles and plots from the dataset\n",
    "def get_all_movie_plots(df):\n",
    "    documents = []\n",
    "    for _, row in df.iterrows():\n",
    "        documents.append({\n",
    "            \"title\": row['Title'],\n",
    "            \"content\": row['Plot']\n",
    "        })\n",
    "    return documents\n",
    "\n",
    "# Fetch all movie plots from the dataset\n",
    "documents = get_all_movie_plots(movies_df)\n",
    "\n",
    "# Save the processed documents to a JSON file\n",
    "with open(\"all_movie_plots_data.json\", \"w\") as f:\n",
    "    json.dump(documents, f)\n",
    "\n",
    "# Display a sample of the documents to verify content\n",
    "#documents[:1]  # Show the first document as an example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a19e810-6ee0-4ca4-93cb-27be086de8a6",
   "metadata": {},
   "source": [
    "# Step 3: Processes the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1c850bf-5ee1-4077-a136-1060f48e67b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize, remove stop words, and apply stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Limit processing to the first 200 documents\n",
    "processed_documents = []\n",
    "\n",
    "for i, doc in enumerate(documents[:200]):  # Only process the first 200 articles\n",
    "    try:\n",
    "        processed_documents.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": preprocess_text(doc[\"content\"])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i + 1}: {e}\")\n",
    "       \n",
    "# Save processed documents incrementally\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Display a sample to verify\n",
    "#processed_documents[:1]  # Show the first processed document as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdf563e-639e-4eb1-94c7-b0793075301d",
   "metadata": {},
   "source": [
    "# Step 3.Î‘: Processes the documents (Includes stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4af75b28-49b7-419d-80aa-17ed071c3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function: Includes stopwords\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize and retain stopwords\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Limit processing to the first 200 documents\n",
    "processed_documents = []\n",
    "\n",
    "for i, doc in enumerate(documents[:200]):  # Only process the first 200 articles\n",
    "    try:\n",
    "        processed_documents.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": preprocess_text(doc[\"content\"])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i + 1}: {e}\")\n",
    "       \n",
    "# Save processed documents incrementally\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Display a sample to verify\n",
    "#processed_documents[:1]  # Show the first processed document as an example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84fc48b-f250-41f6-9e7b-a3f06a73483c",
   "metadata": {},
   "source": [
    "# Step 3.Î’: Processes the documents(Skips stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a1ad3a9-1983-4b66-adf2-c30c7ed7f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function: Skips stemming\n",
    "def preprocess_text(text):\n",
    "    # Lowercase and remove special characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z0-9\\s]\", \"\", text)\n",
    "    \n",
    "    # Tokenize and remove stopwords, without stemming\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Limit processing to the first 200 documents\n",
    "processed_documents = []\n",
    "\n",
    "for i, doc in enumerate(documents[:200]):  # Only process the first 200 articles\n",
    "    try:\n",
    "        processed_documents.append({\n",
    "            \"title\": doc[\"title\"],\n",
    "            \"content\": preprocess_text(doc[\"content\"])\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing document {i + 1}: {e}\")\n",
    "       \n",
    "# Save processed documents incrementally\n",
    "with open(\"processed_data.json\", \"w\") as f:\n",
    "    json.dump(processed_documents, f)\n",
    "\n",
    "# Display a sample to verify\n",
    "#processed_documents[:1]  # Show the first processed document as an example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc9917f-aca4-467d-a68c-3f598cd39ddb",
   "metadata": {},
   "source": [
    "# Step 4: Create inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc77d2e2-fe70-4011-8df6-f162885e753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty inverted index\n",
    "inverted_index = defaultdict(dict)\n",
    "\n",
    "# Populate the inverted index with term frequencies\n",
    "for doc_id, doc in enumerate(processed_documents):\n",
    "    for term in doc[\"content\"]:\n",
    "        if doc_id in inverted_index[term]:\n",
    "            inverted_index[term][doc_id] += 1\n",
    "        else:\n",
    "            inverted_index[term][doc_id] = 1\n",
    "\n",
    "# Save the inverted index to a JSON file\n",
    "with open(\"inverted_index.json\", \"w\") as f:\n",
    "    json.dump(inverted_index, f)\n",
    "\n",
    "# Display a small part of the inverted index to verify content\n",
    "#dict(list(inverted_index.items())[:5])  # Show the first 5 terms in the index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5141b6bd-e265-4d91-a665-c0375a06d805",
   "metadata": {},
   "source": [
    "# Step 5:Boolean Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f5ba9e5b-5696-4c06-a095-1aca9447a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the query with Boolean operators\n",
    "def parse_query(query):\n",
    "    terms = query.lower().split()\n",
    "    tokens = []\n",
    "    operators = {\"and\", \"or\", \"not\"}\n",
    "    \n",
    "    for term in terms:\n",
    "        if term in operators:\n",
    "            tokens.append(term)\n",
    "        else:\n",
    "            processed_term = stemmer.stem(term) if term not in stop_words else \"\"\n",
    "            if processed_term:\n",
    "                tokens.append(processed_term)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Boolean retrieval function\n",
    "def boolean_retrieval(query_tokens):\n",
    "    result_set = set(range(len(processed_documents)))  # Start with all documents\n",
    "    current_operation = \"and\"\n",
    "    \n",
    "    for token in query_tokens:\n",
    "        if token in {\"and\", \"or\", \"not\"}:\n",
    "            current_operation = token\n",
    "        else:\n",
    "            matching_docs = set(inverted_index.get(token, {}).keys())\n",
    "            if current_operation == \"and\":\n",
    "                result_set &= matching_docs\n",
    "            elif current_operation == \"or\":\n",
    "                result_set |= matching_docs\n",
    "            elif current_operation == \"not\":\n",
    "                result_set -= matching_docs\n",
    "    \n",
    "    return list(result_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247a6520-855b-4f74-98fa-43e4ace68acb",
   "metadata": {},
   "source": [
    "# Step 6:TF-IDF Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76e60751-fc45-40c6-804a-8bcead8778c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF weight for a term in a document\n",
    "def compute_tf_idf(term, doc_id):\n",
    "    term_frequency = inverted_index[term].get(doc_id, 0)\n",
    "    if term_frequency == 0:\n",
    "        return 0\n",
    "    document_frequency = len(inverted_index[term])\n",
    "    inverse_document_frequency = math.log(len(processed_documents) / (1 + document_frequency))\n",
    "    return term_frequency * inverse_document_frequency\n",
    "\n",
    "# Rank results by TF-IDF scores\n",
    "def rank_results_tf_idf(query_tokens, result_docs):\n",
    "    doc_scores = {}\n",
    "    for doc_id in result_docs:\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            if term not in {\"and\", \"or\", \"not\"}:\n",
    "                score += compute_tf_idf(term, doc_id)\n",
    "        doc_scores[doc_id] = score\n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891b5101-80e6-400c-81bb-35cc29231b1c",
   "metadata": {},
   "source": [
    "# Step 7:BM25 Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e66731ad-7495-4f19-918e-a90d656e36e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 parameters\n",
    "k1 = 1.5  # Term frequency saturation parameter\n",
    "b = 0.75  # Length normalization parameter\n",
    "\n",
    "# Precompute document lengths and average length\n",
    "document_lengths = [len(doc[\"content\"]) for doc in processed_documents]\n",
    "avg_doc_length = sum(document_lengths) / len(document_lengths)\n",
    "\n",
    "# Function to compute BM25 score for a term in a document\n",
    "def compute_bm25(term, doc_id):\n",
    "    term_frequency = inverted_index[term].get(doc_id, 0)\n",
    "    document_frequency = len(inverted_index[term])\n",
    "    N = len(processed_documents)\n",
    "    \n",
    "    # Calculate IDF component with BM25 modification\n",
    "    idf = math.log((N - document_frequency + 0.5) / (document_frequency + 0.5) + 1)\n",
    "    \n",
    "    # Calculate the BM25 term score\n",
    "    doc_length = document_lengths[doc_id]\n",
    "    tf_component = (term_frequency * (k1 + 1)) / (term_frequency + k1 * (1 - b + b * (doc_length / avg_doc_length)))\n",
    "    \n",
    "    return idf * tf_component\n",
    "\n",
    "# Rank results by BM25 scores\n",
    "def rank_results_bm25(query_tokens, result_docs):\n",
    "    doc_scores = {}\n",
    "    for doc_id in result_docs:\n",
    "        score = 0\n",
    "        for term in query_tokens:\n",
    "            if term not in {\"and\", \"or\", \"not\"}:\n",
    "                score += compute_bm25(term, doc_id)\n",
    "        doc_scores[doc_id] = score\n",
    "    return sorted(doc_scores.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920dd41c-a2fb-4ff4-9fa4-fb692308ae07",
   "metadata": {},
   "source": [
    "# Step 7: VSM Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ec151db-2c52-43d9-89d5-e4f87b4db177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def vector_space_model(query_tokens):\n",
    "   \n",
    "    # Filter documents using Boolean Retrieval results\n",
    "    result_docs = boolean_retrieval(query_tokens)\n",
    "    if not result_docs:  # No documents matched\n",
    "        print(\"No documents match the query.\")\n",
    "        return []\n",
    "\n",
    "    # Extract text for the filtered documents\n",
    "    corpus = [\" \".join(processed_documents[doc_id][\"content\"]) for doc_id in result_docs]\n",
    "    query = \" \".join(query_tokens)  # Represent query as a single string\n",
    "\n",
    "    # Vectorize using TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    all_vectors = vectorizer.fit_transform(corpus + [query])  # Last row is the query vector\n",
    "\n",
    "    document_vectors = all_vectors[:-1]  # Exclude query vector (last)\n",
    "    query_vector = all_vectors[-1]  # Query vector\n",
    "\n",
    "    # Compute cosine similarity using sklearn's cosine_similarity\n",
    "    similarity_scores = cosine_similarity(query_vector, document_vectors).flatten()\n",
    "\n",
    "    # Map scores to document IDs\n",
    "    scored_results = [(result_docs[i], score) for i, score in enumerate(similarity_scores)]\n",
    "\n",
    "    # Sort documents by similarity score in descending order\n",
    "    sorted_results = sorted(scored_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return sorted_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ca5220-b70a-416a-a045-ee4be137a636",
   "metadata": {},
   "source": [
    "# Step 8: Search Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c88fa751-39ed-48be-a2e9-6d8ccec2faa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, algorithm=\"boolean\"):\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    if algorithm == \"boolean\":\n",
    "        # Boolean Retrieval: Display document titles\n",
    "        results = boolean_retrieval(query_tokens)\n",
    "        sorted_results = sorted(results)  # Sort by document ID (ascending)\n",
    "        data = [[doc_id, documents[doc_id]['title']] for doc_id in sorted_results]\n",
    "        print(\"\\nBoolean Retrieval Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\"], tablefmt=\"grid\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"tf-idf\":\n",
    "        # TF-IDF Ranking: Display document titles and scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_tf_idf(query_tokens, result_docs)\n",
    "        # Sort by TF-IDF score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "        data = [[doc_id, documents[doc_id]['title'], score] for doc_id, score in sorted_results]\n",
    "        print(\"\\nTF-IDF Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"bm25\":\n",
    "        # BM25 Ranking: Display document titles and BM25 scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_bm25(query_tokens, result_docs)\n",
    "        # Sort by BM25 score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "        data = [[doc_id, documents[doc_id]['title'], score] for doc_id, score in sorted_results]\n",
    "        print(\"\\nOkapi BM25 Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"BM25 Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"vsm\":\n",
    "        # VSM Ranking: Display document titles and similarity scores\n",
    "        ranked_results = vector_space_model(query_tokens)\n",
    "        # Already sorted by VSM score (descending) within `vector_space_model`\n",
    "        data = [[doc_id, documents[doc_id]['title'], score] for doc_id, score in ranked_results[:10]]  # Top 10 results\n",
    "        print(\"\\nVSM Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"VSM Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return ranked_results\n",
    "\n",
    "    else:\n",
    "        print(\"Unsupported algorithm selected.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c626c6f-2048-4109-a8f9-ac9bed4d1e32",
   "metadata": {},
   "source": [
    "# Search Fuction for CISI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "7dd5e802-b9a6-437b-b9df-e294c788eb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, algorithm=\"boolean\"):\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    if algorithm == \"boolean\":\n",
    "        # Boolean Retrieval: Display document titles (or content in case of CISI)\n",
    "        results = boolean_retrieval(query_tokens)\n",
    "        sorted_results = sorted(results)  # Sort by document ID (ascending)\n",
    "\n",
    "        data = []\n",
    "        for doc_id in sorted_results:\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:100]  # First 100 characters or a preview\n",
    "                data.append([doc_id, title])\n",
    "\n",
    "        print(\"\\nBoolean Retrieval Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\"], tablefmt=\"grid\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"tf-idf\":\n",
    "        # TF-IDF Ranking: Display document titles and scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_tf_idf(query_tokens, result_docs)\n",
    "        # Sort by TF-IDF score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        data = []\n",
    "        for doc_id, score in sorted_results:\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:500]  # First 500 characters or a preview\n",
    "                data.append([doc_id, title, score])\n",
    "\n",
    "        print(\"\\nTF-IDF Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"bm25\":\n",
    "        # BM25 Ranking: Display document titles and BM25 scores\n",
    "        result_docs = boolean_retrieval(query_tokens)\n",
    "        ranked_results = rank_results_bm25(query_tokens, result_docs)\n",
    "        # Sort by BM25 score (descending)\n",
    "        sorted_results = sorted(ranked_results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        data = []\n",
    "        for doc_id, score in sorted_results:\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:500]  # First 500 characters or a preview\n",
    "                data.append([doc_id, title, score])\n",
    "\n",
    "        print(\"\\nOkapi BM25 Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"BM25 Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return sorted_results\n",
    "\n",
    "    elif algorithm == \"vsm\":\n",
    "        # VSM Ranking: Display document titles and similarity scores\n",
    "        ranked_results = vector_space_model(query_tokens)\n",
    "        # Already sorted by VSM score (descending) within `vector_space_model`\n",
    "        \n",
    "        data = []\n",
    "        for doc_id, score in ranked_results[:10]:  # Top 10 results\n",
    "            if doc_id in documents:\n",
    "                if isinstance(documents[doc_id], dict) and 'title' in documents[doc_id]:\n",
    "                    title = documents[doc_id]['title']\n",
    "                else:\n",
    "                    # Handle CISI dataset (simple document content as title)\n",
    "                    title = documents[doc_id][:500]  # First 500 characters or a preview\n",
    "                data.append([doc_id, title, score])\n",
    "\n",
    "        print(\"\\nVSM Ranked Results:\")\n",
    "        print(tabulate(data, headers=[\"Document ID\", \"Title\", \"VSM Score\"], tablefmt=\"grid\", floatfmt=\".4f\"))\n",
    "        return ranked_results\n",
    "\n",
    "    else:\n",
    "        print(\"Unsupported algorithm selected.\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cd96d0-7699-4d91-9064-467693d0c822",
   "metadata": {},
   "source": [
    "# Step 9: Interactive Query Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "11ebfb28-b203-47e8-a35b-e7a5f136ffdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Document Retrieval System ---\n",
      "Choose an algorithm:\n",
      "1. Boolean Retrieval\n",
      "2. TF-IDF\n",
      "3. Okapi BM25\n",
      "4. vsm\n",
      "0. Exit\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your choice (0 to exit):  0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exiting\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    while True:\n",
    "        print(\"\\n--- Document Retrieval System ---\")\n",
    "        print(\"Choose an algorithm:\")\n",
    "        print(\"1. Boolean Retrieval\")\n",
    "        print(\"2. TF-IDF\")\n",
    "        print(\"3. Okapi BM25\")\n",
    "        print(\"4. vsm\")\n",
    "        print(\"0. Exit\")\n",
    "\n",
    "        choice = input(\"Enter your choice (0 to exit): \").strip()\n",
    "\n",
    "        if choice == \"0\":\n",
    "            print(\"Exiting\")\n",
    "            break\n",
    "\n",
    "        query = input(\"Enter your search query: \").strip()\n",
    "\n",
    "        if not query:\n",
    "            print(\"Query cannot be empty. Please try again.\")\n",
    "            continue\n",
    "\n",
    "        if choice == \"1\":\n",
    "            print(\"\\nYou selected: Boolean Retrieval\")\n",
    "            search_documents(query, \"boolean\")\n",
    "        elif choice == \"2\":\n",
    "            print(\"\\nYou selected: TF-IDF Ranking\")\n",
    "            search_documents(query, \"tf-idf\")\n",
    "        elif choice == \"3\":\n",
    "            print(\"\\nYou selected: Okapi BM25\")\n",
    "            search_documents(query, \"bm25\")\n",
    "        elif choice == \"4\":\n",
    "            print(\"\\nYou selected: Vector Space Model (VSM)\")\n",
    "            search_documents(query, \"vsm\")\n",
    "        else:\n",
    "            print(\"Invalid choice. Please enter a number between 0 and 4.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "440c915f-98f7-452c-9285-234a5cffdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating with Boolean Retrieval:\n",
      "\n",
      "Evaluating query 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "\n",
      "Boolean Retrieval Results:\n",
      "+---------------+---------+\n",
      "| Document ID   | Title   |\n",
      "+===============+=========+\n",
      "+---------------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 2: How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n",
      "\n",
      "Boolean Retrieval Results:\n",
      "+---------------+---------+\n",
      "| Document ID   | Title   |\n",
      "+===============+=========+\n",
      "+---------------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 3: What is information science?  Give definitions where possible.\n",
      "\n",
      "Boolean Retrieval Results:\n",
      "+---------------+---------+\n",
      "| Document ID   | Title   |\n",
      "+===============+=========+\n",
      "+---------------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Mean Average Precision (MAP): 0.0000\n",
      "\n",
      "Evaluating with TF-IDF Retrieval:\n",
      "\n",
      "Evaluating query 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "\n",
      "TF-IDF Ranked Results:\n",
      "+---------------+---------+---------+\n",
      "| Document ID   | Title   | Score   |\n",
      "+===============+=========+=========+\n",
      "+---------------+---------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 2: How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n",
      "\n",
      "TF-IDF Ranked Results:\n",
      "+---------------+---------+---------+\n",
      "| Document ID   | Title   | Score   |\n",
      "+===============+=========+=========+\n",
      "+---------------+---------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 3: What is information science?  Give definitions where possible.\n",
      "\n",
      "TF-IDF Ranked Results:\n",
      "+---------------+---------+---------+\n",
      "| Document ID   | Title   | Score   |\n",
      "+===============+=========+=========+\n",
      "+---------------+---------+---------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Mean Average Precision (MAP): 0.0000\n",
      "\n",
      "Evaluating with BM25 Retrieval:\n",
      "\n",
      "Evaluating query 1: What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\n",
      "\n",
      "Okapi BM25 Ranked Results:\n",
      "+---------------+---------+--------------+\n",
      "| Document ID   | Title   | BM25 Score   |\n",
      "+===============+=========+==============+\n",
      "+---------------+---------+--------------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 2: How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\n",
      "\n",
      "Okapi BM25 Ranked Results:\n",
      "+---------------+---------+--------------+\n",
      "| Document ID   | Title   | BM25 Score   |\n",
      "+===============+=========+==============+\n",
      "+---------------+---------+--------------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Evaluating query 3: What is information science?  Give definitions where possible.\n",
      "\n",
      "Okapi BM25 Ranked Results:\n",
      "+---------------+---------+--------------+\n",
      "| Document ID   | Title   | BM25 Score   |\n",
      "+===============+=========+==============+\n",
      "+---------------+---------+--------------+\n",
      "Precision: 0.0000, Recall: 0.0000, F1-Score: 0.0000, Average Precision: 0.0000\n",
      "\n",
      "Mean Average Precision (MAP): 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Sample queries (CISI dataset)\n",
    "queries = {\n",
    "    \"1\": \"What problems and concerns are there in making up descriptive titles? What difficulties are involved in automatically retrieving articles from approximate titles? What is the usual relevance of the content of articles to their titles?\",\n",
    "    \"2\":\"How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?\",\n",
    "    \"3\":\"What is information science?  Give definitions where possible.\"\n",
    "}\n",
    "\n",
    "# Sample relevance data \n",
    "# Format: query_id -> list of relevant document IDs (str)\n",
    "relevances = {\n",
    "    \"1\": [\"28\", \"35\", \"38\", \"42\"],  # Relevant document IDs for query 1\n",
    "    \"2\": [\"29\",\"68\",\"197\"],\n",
    "    \"3\": [\"60\", \"85\"]\n",
    "}\n",
    "\n",
    "# Evaluation metrics\n",
    "def compute_average_precision(retrieved_docs, relevant_docs):\n",
    "    num_relevant_retrieved = 0\n",
    "    ap = 0.0\n",
    "\n",
    "    for rank, doc_id in enumerate(retrieved_docs, start=1):\n",
    "        if doc_id in relevant_docs:\n",
    "            num_relevant_retrieved += 1\n",
    "            precision_at_rank = num_relevant_retrieved / rank\n",
    "            ap += precision_at_rank\n",
    "\n",
    "    if len(relevant_docs) > 0:\n",
    "        ap /= len(relevant_docs)\n",
    "    \n",
    "    return ap\n",
    "\n",
    "def evaluate_search_results(retrieved_docs, query_id, relevances):\n",
    "    relevant_docs = relevances[query_id]\n",
    "    tp = len(set(retrieved_docs) & set(relevant_docs))  # True Positives\n",
    "    fp = len(set(retrieved_docs) - set(relevant_docs))  # False Positives\n",
    "    fn = len(set(relevant_docs) - set(retrieved_docs))  # False Negatives\n",
    "\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "def evaluate_all_queries(queries, relevances, search_func, algorithm=\"boolean\"):\n",
    "    map_score = 0\n",
    "    for query_id, query_content in queries.items():\n",
    "        print(f\"\\nEvaluating query {query_id}: {query_content}\")\n",
    "        \n",
    "        retrieved_docs = search_func(query_content, algorithm=algorithm)\n",
    "\n",
    "        precision, recall, f1 = evaluate_search_results(retrieved_docs, query_id, relevances)\n",
    "        ap = compute_average_precision(retrieved_docs, relevances[query_id])\n",
    "        map_score += ap\n",
    "\n",
    "        print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}, Average Precision: {ap:.4f}\")\n",
    "\n",
    "    map_score /= len(queries)\n",
    "    print(f\"\\nMean Average Precision (MAP): {map_score:.4f}\")\n",
    "\n",
    "# Evaluation for each algorithm\n",
    "print(\"\\nEvaluating with Boolean Retrieval:\")\n",
    "evaluate_all_queries(queries, relevances, search_documents, algorithm=\"boolean\")\n",
    "print(\"\\nEvaluating with TF-IDF Retrieval:\")\n",
    "evaluate_all_queries(queries, relevances, search_documents, algorithm=\"tf-idf\")\n",
    "print(\"\\nEvaluating with BM25 Retrieval:\")\n",
    "evaluate_all_queries(queries, relevances, search_documents, algorithm=\"bm25\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db34dc-7d2d-4bf0-b721-539dd0569eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6489358f-b1f6-401f-a2e8-e050ae3704df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
